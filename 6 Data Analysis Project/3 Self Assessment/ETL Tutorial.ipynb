{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ETL Tutorial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data engineers use ETL (Extract-Transform-Load) processes widely in data warehouses to move data between databases, servers, and machines. ETL processes are an intersection of process engineering and technology. It is important to think about ETL processes as actual processes and not as physical implementations of the data.\n",
    "\n",
    "An ETL process can be used in the following situations:\n",
    "\n",
    "* To access data in a source database or other storage location and load it into a different database or storage location. This process is equivalent to a simple copy and paste of the data from source to target.\n",
    "* To access data in the source database or storage, perform some transformation to meet the schema of the target table, and then store the data in the target table. An example of this situation is extracting data from an OLTP database, performing some transformation on the data to meet the schema of the target database, then loading the transformed data into the target database.\n",
    "* To extract data that is not stored in a database and move that data to a database. For example, this could be data stored in text files, spreadsheets, or similar unorganized files. The data is transformed into a format that can be stored in a temporary database and then loaded to the target database."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Design and Implement Custom ETL Scripts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the most common tasks for a data engineer is designing and implementing ETL processes. In this tutorial, we will learn how to design and develop a simple ETL tool.\n",
    "\n",
    "As discussed above, an ETL process has three steps: extract, transform, and load.\n",
    "\n",
    "Before we can start to build an ETL script, we must establish the requirements for our ETL tool. Below is a list of requirements that we will start with.\n",
    "\n",
    "\n",
    "The extract step must be able to access data from a variety of data sources, including:\n",
    "\n",
    "* Text files\n",
    "* CSV files\n",
    "* JSON files\n",
    "\n",
    "The transform step must support several transformations on the data, including:\n",
    "\n",
    "* Resizing data and reshaping rows (e.g., selecting a limited number of columns from the original data)\n",
    "* Converting and parsing data (e.g., converting a string to an integer or parsing the numeric part of a string)\n",
    "* Transforming columns\n",
    "* Manipulating headers (e.g., changing the column headers)\n",
    "* Sorting data\n",
    "* Grouping data\n",
    "* Concatenating data\n",
    "* Detecting and removing duplicate data\n",
    "* Filling in missing values and replacing erroneous values\n",
    "\n",
    "The load step must support saving data to a variety of data formats, including:\n",
    "\n",
    "* Text files\n",
    "* CSV files\n",
    "* JSON files\n",
    "\n",
    "The script must support adding new extract, transform, and load functions. For instance, in the future, we might add an Oracle SQL server to our data infrastructure and then we would need to write additional extract and load functions to store and retrieve data on that server. \n",
    "\n",
    "To this end, we must use object-oriented and functional programming to design a flexible ETL tool.\n",
    "\n",
    "In this project, you will build a template for a simple ETL library as well as implement extract, transform, and load capabilities.\n",
    "\n",
    "You may use any files you wish for the initial datasets, including files provided for this course, files you find online, or files you create yourself. It is also useful to test using at least two different datasets for each file type to be certain that your code is not file-specific."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Design and Implement the Extract Class\n",
    "\t\n",
    "In the first step, we will **extract** data from an existing data source and convert it to a format that we can use in the transform steps. Specifically, in this project, we will take a structured dataset, which includes attributes and values, and convert it into a new dataset that includes a list of dictionaries for each record in the original dataset. Extracting the data and converting it to a standard format will make it easy to create transform methods to save the data in a different format later.\n",
    "\n",
    "The code below provides a template for the `extract` class, including methods for common file types we know we will use.\n",
    "\n",
    "    class extract:\n",
    "    def fromCSV(self):\n",
    "        pass\n",
    "    def fromJSON(self):\n",
    "        pass\n",
    "\n",
    "This structure allows us to easily add additional methods if we later need to extract from a different data source."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instructions\n",
    "\n",
    "\t\n",
    "Implement each method in the `extract` class, following these requirements: \n",
    "\n",
    "* Do not change any method names. \n",
    "* You can add any number of inputs to each method. \n",
    "* Handle appropriate exceptions for missing files. \n",
    "* Each method should return a list.\n",
    "  * Each element in the list should be a dictionary.\n",
    "  * Each dictionary should use the attribute name as the key with its corresponding value from the original dataset.\n",
    "  * For example, it should look like the dataset extracted using the `fromCSV` method below. \n",
    "* Test each method with various files to make sure that the methods are working as expected.\n",
    "\n",
    "For this example, we use a CSV file, [got_chars.csv](https://the-software-guild.s3.amazonaws.com/python/v1-1911/data-files/got_chars.csv), but you can use any available CSV file to test it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class extract:\n",
    "    def fromCSV(self):\n",
    "        pass\n",
    "    def fromJSON(self):\n",
    "        pass\n",
    "    \n",
    "\n",
    "e = extract()\n",
    "dataset = e.fromCSV(file_path=\"data/got_chars.csv\")\n",
    "for row in dataset:\n",
    "    print(row)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output should be organized into lists, where each list is a collection of dictionaries. For the *got_chars.csv* file, the output should look like this:\n",
    "\n",
    "    OrderedDict([('actor', 'Sean Bean'), ('character', '\"Eddard \"\"Ned\"\" Stark\"'), ('first_appearance', '1')])\n",
    "    OrderedDict([('actor', 'Mark Addy'), ('character', 'Robert Baratheon'), ('first_appearance', '1')])\n",
    "    OrderedDict([('actor', 'Nikolaj Coster-Waldau'), ('character', 'Jaime Lannister'), ('first_appearance', '1')])\n",
    "    OrderedDict([('actor', 'Michelle Fairley'), ('character', 'Catelyn Stark'), ('first_appearance', '1')])\n",
    "    OrderedDict([('actor', 'Lena Headey'), ('character', 'Cersei Lannister'), ('first_appearance', '1')])\n",
    "    OrderedDict([('actor', 'Emilia Clarke'), ('character', 'Daenerys Targaryen'), ('first_appearance', '1')])\n",
    "\n",
    "        [ â€¦ ]\n",
    "\n",
    "    OrderedDict([('actor', 'Dean-Charles Chapman[d]'), ('character', 'Tommen Baratheon'), ('first_appearance', '1')])\n",
    "    OrderedDict([('actor', 'Tom Wlaschiha[e]'), ('character', \"Jaqen H'ghar\"), ('first_appearance', '1')])\n",
    "    OrderedDict([('actor', 'Michael McElhatton'), ('character', 'Roose Bolton'), ('first_appearance', '2')])\n",
    "    OrderedDict([('actor', 'Jonathan Pryce'), ('character', 'The High Sparrow'), ('first_appearance', '5')])\n",
    "    OrderedDict([('actor', 'Jacob Anderson'), ('character', 'Grey Worm'), ('first_appearance', '3')])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When the `extract` class works as expected, save it as *extract.py* in the same directory you are using for this project. \n",
    "\n",
    "* Create a new file named *extract.py* using any text editor.\n",
    "* Copy **only** the code for the `extract` class (without the code that implements the class), and paste it into the new file.\n",
    "\n",
    "This process creates an external class that we can reuse in other scripts. To call the external class, include the following command on the first line of the new script:\n",
    "\n",
    "    from extract import extract\n",
    "\n",
    "Save the file you used to create and test the class separately (as a Jupyter Notebook file or as a Python file with a different name), in case you need to refer to it again later."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Design and Implement the Transform Class\n",
    "\n",
    "Once the `extract` class works as expected and we have the *extract.py* module ready to use, we can start building the `transform` class.\n",
    "\n",
    "Data engineering processes frequently involve moving data from one storage structure to another. For example, we may have existing data stored as a CSV or JSON file, but the data scientists we are working with want the data stored in a relational database like MySQL or SQL Server.\n",
    "\n",
    "For this reason, the second step of the ETL process is **transform**. This means that after we extract data from the source files, we change the data to match the target specifications and store the transformed data in a temporary location until we can load it into the target storage structure. Transformation can include renaming columns, removing columns we do not need, and modifying the data itself.\n",
    "\n",
    "As with the earlier extract steps, we want to automate this process as much as possible, so we will build a Python class that can perform these steps for us and save the class as an external Python module."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instructions\n",
    "Using a template like the one below, create a series of transform functions, each of which should perform the steps described in the comments.\n",
    "\n",
    "* Implement each method in the `transform` class. \n",
    "  * Each method in must take as input the dataset generated from the extract step (a list of dictionaries). \n",
    "* Implement appropriate exception handling for predictable errors.\n",
    "* Use lambda functions, `map`, `reduce`, and `filter` where appropriate \n",
    "  * For example, we can use a `map` function and a lambda function to implement the `rename_attribute` function.\n",
    "* Each method must take as input a dataset consisting of a list of dictionaries (as output by the `extract` class) and return another list of dictionaries dataset. \n",
    "  * Add any other appropriate input for each method. \n",
    "* Test each method in the transform class to make sure they work correctly.\n",
    "\n",
    "Once the code works as expected, save the transform class in a new file *transform.py* using the same steps used for the extract class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from extract import extract # import the external extract class\n",
    "\n",
    "class transform:\n",
    "    def head(self, dataset, step): #return the top N records from the dataset\n",
    "        pass\n",
    "    def tail(self): #return the last N records from the dataset\n",
    "        pass\n",
    "    def rename_attribute(self): #rename a column in the dataset\n",
    "        pass\n",
    "    def remove_attribute(self): #remove a column from the dataset\n",
    "        pass\n",
    "    def rename_attributes(self): #rename a list of columns in the dataset\n",
    "        pass\n",
    "    def remove_attributes(self): #remove a list of columns in the dataset\n",
    "        pass\n",
    "    def transform(self):\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Design and Implement the Load class\n",
    "\n",
    "The final step of an ETL process is to **load** the transformed data into a relatively permanent storage location so that data scientists and other users can access the data and use it as needed.\n",
    "\n",
    "As with the `extract` class we've already defined, we want our ETL script to be able to put the transformed data into CSV and JSON formats. In the process, we will save the transformed data to an external file.\n",
    "\n",
    "Note that the data format we use to load the data does not have to be the same as the format we started with. In fact, it is common for an ETL process to change the data format to better meet the needs of the data scientists who will actually use the data. \n",
    "\n",
    "In this example, we use the `extract` class to import data from the existing *[citigroup.csv](https://the-software-guild.s3.amazonaws.com/python/v1-1911/data-files/citigroup.csv)* file to a copy of that file, also in CSV format.\n",
    "\n",
    "Because this is a simple example, we do not actually transform the data in any way here, and the result should be an exact copy of the file we started with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from extract import extract # import the external extract class\n",
    "\n",
    "class load:\n",
    "    def toCSV(self):\n",
    "        pass\n",
    "    def toJSON(self):\n",
    "        pass\n",
    "    \n",
    "\n",
    "e = extract()\n",
    "dataset = e.fromCSV(file_path = 'data/citigroup.csv',delimiter = ',')\n",
    " \n",
    "l = load()\n",
    "l.toCSV(file_path = \"data/citigroup_copy.csv\", dataset = dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instructions\n",
    "Using the template code above as a starting point, complete the following steps:\n",
    "\n",
    "\n",
    "* Implement the methods in the `load` class. \n",
    "* Implement necessary exceptions to  handle missing files. \n",
    "* Each method should take as input a variable `file_path` that contains the target file to store the loaded data.\n",
    "* Each method should take as input a variable `dataset` that is a list of dictionaries containing the data we want to store. \n",
    "* Test the `load` class to make sure it works properly to output both CSV and JSON files. \n",
    "* Save the `load` class in a file *load.py*. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Implement an ETL Script\n",
    "\n",
    "Now that we have each of the individual steps working, let's put them together into a single script that we can use to automate an ETL process. \n",
    "\n",
    "This script must perform the following steps:\n",
    "\n",
    "* Call each of the `extract`, `transform`, and `load` classes you have already created.\n",
    "* Input a file using the appropriate `extract` method.\n",
    "* Transform the data in some way. \n",
    "  * Use one or more of the `transform` methods as appropriate for the dataset you are using.\n",
    "* Load the results into a new CSV or JSON file.\n",
    "\n",
    "Test the results using both CSV and JSON files for the input and output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from extract import extract # import the external extract class\n",
    "from transform import transform # import the external extract class\n",
    "from load import load # import the external load class\n",
    "\n",
    "# your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Implement Your Own Extract and Load Methods\n",
    "\n",
    "When we created the `extract` and `load` classes above, we started with a template that allows us to add additional file types as necessary. Now, let's see how to do that.\n",
    "\n",
    "* Research and identify at least one other type of data file to include in the `extract` and `load` classes.\n",
    "  * Examples:\n",
    "    * XML\n",
    "    * Excel\n",
    "    * API \n",
    "* Use a search engine like Google to research how to read data from the chosen data source. \n",
    "  * Identify any Python library that you might need.\n",
    "    * For example, Python offers the `ElementTree` module to read XML data. \n",
    "* Implement the `extract` and `load` methods using the the new data source type(s).\n",
    "  * Use your original files from Step 1 and Step 3 to ensure that the `extract` and `load` classes work appropriately.\n",
    "  * Make changes only to the individual class files at this point, not to the ETL script created in the previous step.\n",
    "\n",
    "  * The name of the `extract` and `load` methods must include the name of the source, as in the CSV and JSON models. \n",
    "    * For example, if the source is an Excel file, we can use `fromExcel` as the method name. If the output is an Excel file, the method name should be `toExcel`. \n",
    "  * The extract method must return a list of dictionaries, similar to the `fromCSV` and `fromJSON` methods. \n",
    "  * Add any appropriate inputs to the method. \n",
    "  * Implement the necessary exception handling. \n",
    "  * Test the new code using multiple files and file types.\n",
    "\n",
    "After you have tested the updated classes and everything works, update the *extract.py* and *load.py* files with the new code and test your script from the previous step to make sure that everything works correctly with the new file type(s)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
